{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가\n",
    "\n",
    "### 기존 개발 vs AI 개발\n",
    "- 기존 개발 : 유닛 테스트, E2E 테스트 -> QA (품질보증, Quality Assurance)\n",
    "- AI 개발 : 모델 평가 -> QA\n",
    "\n",
    "### 모델 학습 및 배포 과정\n",
    "- 데이터 취득 -> 모델 학습 -> 모델 평가 -> 모델 배포\n",
    "- 모델 평가\n",
    "  - 모델 검증\n",
    "  - 모델 성능 지표 분석\n",
    "  - 사용자 동작 테스트\n",
    "  - 모델 모니터링 (off line (online 되기 전), online의 경우 실제 배포 후 성능지표)\n",
    "\n",
    "### 모델 평가 과정\n",
    "- Model Validation (학습)\n",
    "- Model Evaluation\n",
    "  - 모델 출력 분포 다양한 매트릭으로 정밀 분석\n",
    "- Behavior Testing\n",
    "  - 시나리오 테스트\n",
    "  - 사용자 결과 테스트\n",
    "\n",
    "\n",
    "### 주요 평가 메트릭\n",
    "- 정확도 (Accuracy)\n",
    "- F1 Score\n",
    "  - 정밀도, 재현율 적절히 혼용\n",
    "  - 모델의 정확도가 높더라도 샘플수가 적어서 편향될 수 있다.\n",
    "- 정밀도 (Precision)\n",
    "- 재현율 (Recall)\n",
    "  - 모델 평가한 것 중 제대로 맞는 비율은?\n",
    "  - 잘못 예측한 Positive를 줄이는 것이 목적\n",
    "- 분류 성능 평가 지표\n",
    "  - 실제 Positive 데이터 중 모델이 맞춘 비율은 어느정도?\n",
    "  - 모델이 Positive 놓치는 것이 없도록 집중\n",
    "- 드리프트 (Drift)\n",
    "  - 모델이 잘 학습되었다 하더라도 실제 분포와 다를 수 있는데, 이 차이를 드리프트(Drift)라고 부른다.\n",
    "\n",
    "## 분류 모델 학습 및 평가 실습\n",
    "\n",
    "### 분류 모델\n",
    "- 뉴스 카테고리 분류\n",
    "- 과정\n",
    "  - AG News 입력 -> BertForSequenceClassification -> 0 World, 1 Sports, 2 Business, 3 Sci/Tech\n",
    "- BertForSequenceClassification\n",
    "  - Bert는 경량데이터, 12만개중에서 1200개만 학습 돌릴 예정\n",
    "- huggingface\n",
    "  - 모델 및 데이터 등록 사이트 (GitHub와 비슷)\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "```mermaid\n",
    "A_1[\"데이터 셋 전처리\"]-->B[\"모델 학습\"]-->C[\"모델 평가\"]\n",
    "A_2[\"BERT 모델 다운로드\"]-->B\n",
    "C-->C_1[\"Accuracy 평가\"]\n",
    "C-->C_2[\"Precision, Recall, F1 Score 평가\"]\n",
    "C-->C_3[\"Confusion Matrix 평가\"]\n",
    "C-->C_4[\"ROC Curve 평가\"]\n",
    "```\n",
    "- ROC Curve\n",
    "  - Threshold에 따라 True Positive Rate(TPR)와 False Positive Rate(FPR)을 바탕으로 나타낸 모델 성능 그래프\n",
    "  - 그래프 밑면의 넓이가 넓을 수록 모델 성능이 좋음\n",
    "  - 대각선은 랜덤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n",
    "\n",
    "### 모델 학습 Error\n",
    "ValueError: Unable to avoid copy while creating an array as requested.\n",
    "If using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - anaconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local\n",
      "\n",
      "  added / updated specs:\n",
      "    - datasets\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  aiohappyeyeballs   pkgs/main/linux-64::aiohappyeyeballs-2.4.4-py311h06a4308_0 \n",
      "  aiohttp            pkgs/main/linux-64::aiohttp-3.11.10-py311h5eee18b_0 \n",
      "  aiosignal          pkgs/main/noarch::aiosignal-1.2.0-pyhd3eb1b0_0 \n",
      "  arrow-cpp          pkgs/main/linux-64::arrow-cpp-19.0.0-h865e1df_0 \n",
      "  attrs              pkgs/main/linux-64::attrs-24.3.0-py311h06a4308_0 \n",
      "  aws-c-auth         pkgs/main/linux-64::aws-c-auth-0.6.19-h5eee18b_0 \n",
      "  aws-c-cal          pkgs/main/linux-64::aws-c-cal-0.5.20-hdbd6064_0 \n",
      "  aws-c-common       pkgs/main/linux-64::aws-c-common-0.8.5-h5eee18b_0 \n",
      "  aws-c-compression  pkgs/main/linux-64::aws-c-compression-0.2.16-h5eee18b_0 \n",
      "  aws-c-event-stream pkgs/main/linux-64::aws-c-event-stream-0.2.15-h6a678d5_0 \n",
      "  aws-c-http         pkgs/main/linux-64::aws-c-http-0.6.25-h5eee18b_0 \n",
      "  aws-c-io           pkgs/main/linux-64::aws-c-io-0.13.10-h5eee18b_0 \n",
      "  aws-c-mqtt         pkgs/main/linux-64::aws-c-mqtt-0.7.13-h5eee18b_0 \n",
      "  aws-c-s3           pkgs/main/linux-64::aws-c-s3-0.1.51-hdbd6064_0 \n",
      "  aws-c-sdkutils     pkgs/main/linux-64::aws-c-sdkutils-0.1.6-h5eee18b_0 \n",
      "  aws-checksums      pkgs/main/linux-64::aws-checksums-0.1.13-h5eee18b_0 \n",
      "  aws-crt-cpp        pkgs/main/linux-64::aws-crt-cpp-0.18.16-h6a678d5_0 \n",
      "  aws-sdk-cpp        pkgs/main/linux-64::aws-sdk-cpp-1.11.212-hecad206_0 \n",
      "  dill               pkgs/main/linux-64::dill-0.3.8-py311h06a4308_0 \n",
      "  frozenlist         pkgs/main/linux-64::frozenlist-1.5.0-py311h5eee18b_0 \n",
      "  gflags             pkgs/main/linux-64::gflags-2.2.2-h6a678d5_1 \n",
      "  glog               pkgs/main/linux-64::glog-0.5.0-h6a678d5_1 \n",
      "  libabseil          pkgs/main/linux-64::libabseil-20240116.2-cxx17_h6a678d5_0 \n",
      "  libbrotlicommon    pkgs/main/linux-64::libbrotlicommon-1.0.9-h5eee18b_9 \n",
      "  libbrotlidec       pkgs/main/linux-64::libbrotlidec-1.0.9-h5eee18b_9 \n",
      "  libbrotlienc       pkgs/main/linux-64::libbrotlienc-1.0.9-h5eee18b_9 \n",
      "  libevent           pkgs/main/linux-64::libevent-2.1.12-hdbd6064_1 \n",
      "  libgrpc            pkgs/main/linux-64::libgrpc-1.62.2-h2d74bed_0 \n",
      "  libprotobuf        pkgs/main/linux-64::libprotobuf-4.25.3-he621ea3_0 \n",
      "  libthrift          pkgs/main/linux-64::libthrift-0.15.0-h1795dd8_2 \n",
      "  multidict          pkgs/main/linux-64::multidict-6.1.0-py311h5eee18b_0 \n",
      "  multiprocess       pkgs/main/linux-64::multiprocess-0.70.15-py311h06a4308_0 \n",
      "  orc                pkgs/main/linux-64::orc-2.0.1-h2d29ad5_0 \n",
      "  propcache          pkgs/main/linux-64::propcache-0.2.0-py311h5eee18b_0 \n",
      "  pyarrow            pkgs/main/linux-64::pyarrow-19.0.0-py311h6a678d5_1 \n",
      "  re2                pkgs/main/linux-64::re2-2022.04.01-h295c915_0 \n",
      "  s2n                pkgs/main/linux-64::s2n-1.3.27-hdbd6064_0 \n",
      "  snappy             pkgs/main/linux-64::snappy-1.2.1-h6a678d5_0 \n",
      "  utf8proc           pkgs/main/linux-64::utf8proc-2.6.1-h5eee18b_1 \n",
      "  yarl               pkgs/main/linux-64::yarl-1.18.0-py311h5eee18b_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%conda uninstall numpy\n",
    "#%conda install numpy<2\n",
    "#%pip install --upgrade jupyter ipywidgets\n",
    "#%conda install markupsafe\n",
    "#%pip install --upgrade multiprocess\n",
    "%conda install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - anaconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pyyaml             pkgs/main/linux-64::pyyaml-6.0.2-py311h5eee18b_0 \n",
      "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - anaconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      " - anaconda\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets # huggingface에서 데이터 가져옮\n",
    "%conda install transformers\n",
    "%pip install -U scikit-learn\n",
    "# %pip install -U scikit-image\n",
    "%pip install -U matplotlib\n",
    "%conda install seaborn\n",
    "%conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Pool' from 'multiprocess' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, AdamW \u001b[38;5;66;03m# huggingface에서 bert모델가져올때 씀\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m# progress 시각화\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict \u001b[38;5;66;03m# typing, mytyping 유지보수 좋게 해줌\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.19.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py:72\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CommitInfo,\n\u001b[1;32m     65\u001b[0m     CommitOperationAdd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     HfApi,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RepoFile\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Pool' from 'multiprocess' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # 좀더 복잡한 시각화\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW # huggingface에서 bert모델가져올때 씀\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm # progress 시각화\n",
    "from typing import TypedDict # typing, mytyping 유지보수 좋게 해줌\n",
    "\n",
    "# 머신러닝 함수 제공\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AG_News\n",
    "- Dataset\n",
    "  - https://huggingface.co/dtasets/ag_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Base Uncased Model\n",
    "- 110M param (경량)\n",
    "- https://huggingface.co/bert-base-uncased\n",
    "  - <img src=\"https://www.researchgate.net/profile/Mashrin-Srivastava/publication/357987621/figure/fig2/AS:1114523993948163@1642734635345/Top-Left-BERT-encoder-with-bert-base-uncased-backend-Bottom-Left-Encoding-meta.ppm\">\n",
    "- image link: https://www.researchgate.net/figure/Top-Left-BERT-encoder-with-bert-base-uncased-backend-Bottom-Left-Encoding-meta_fig2_357987621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # 학습을 위한 문자 인코딩\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetItem(TypedDict): # Dictiornay를 TypedDict로 정의 가능\n",
    "    text:str\n",
    "    label:str\n",
    "\n",
    "def preprocess_data(dataset_item: DatasetItem) -> dict[str, torch.Tensor]: # dictionary는 각각 타입을 지정해줘야 한다.\n",
    "    return tokenizer(dataset_item[\"text\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # dataset_item.text와 같이 쉽게 데이터 바로 사용가능 (단점 리소스가 많이 듦)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(1200)).map(preprocess_data, batched=True)\n",
    "test_dataset = dataset[\"test\"].select(range(800)).map(preprocess_data, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 완료 데이터\n",
    "- label : 뉴스 분류\n",
    "- input_ids : 토큰화된 텍스트 데이터\n",
    "- token_type_ids : 여러 문장에 한하여, Bert에 입력되는 두개의 시퀀스를 구분 (모두 0으로 적용)\n",
    "- attension_mask : 시퀀스의 각 위치가 실제 데이터인지, 패딩을 위한 추가부분인지를 나타내는 마스크 (실제 1, 패딩은 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2813, 2358, 1012, 6468, 15020, 2067, 204...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 18431, 2571, 3504, 2646, 3293, 13395, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 3514, 1998, 4610, 6112, 15768, 1005, 176...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 5712, 9190, 2015, 3514, 14338, 2013, 236...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 3514, 7597, 2061, 2906, 2000, 2035, 1011...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>Airline Looks to Delay Its Pension Payments In...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 8582, 3504, 2000, 8536, 2049, 11550, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>McDonald's Goes for Gold For a four-year inves...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 9383, 1005, 1055, 3632, 2005, 2751, 2005...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Auction for Shares In Google's IPO May End Tod...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 10470, 2005, 6661, 1999, 8224, 1005, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Four Firms Buy Intelsat Intelsat Ltd., the pio...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 2176, 9786, 4965, 13420, 16846, 13420, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>Auction for Shares in Google's IPO May End Goo...</td>\n",
       "      <td>3</td>\n",
       "      <td>[101, 10470, 2005, 6661, 1999, 8224, 1005, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     Wall St. Bears Claw Back Into the Black (Reute...      2   \n",
       "1     Carlyle Looks Toward Commercial Aerospace (Reu...      2   \n",
       "2     Oil and Economy Cloud Stocks' Outlook (Reuters...      2   \n",
       "3     Iraq Halts Oil Exports from Main Southern Pipe...      2   \n",
       "4     Oil prices soar to all-time record, posing new...      2   \n",
       "...                                                 ...    ...   \n",
       "1195  Airline Looks to Delay Its Pension Payments In...      2   \n",
       "1196  McDonald's Goes for Gold For a four-year inves...      2   \n",
       "1197  Auction for Shares In Google's IPO May End Tod...      3   \n",
       "1198  Four Firms Buy Intelsat Intelsat Ltd., the pio...      3   \n",
       "1199  Auction for Shares in Google's IPO May End Goo...      3   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [101, 2813, 2358, 1012, 6468, 15020, 2067, 204...   \n",
       "1     [101, 18431, 2571, 3504, 2646, 3293, 13395, 10...   \n",
       "2     [101, 3514, 1998, 4610, 6112, 15768, 1005, 176...   \n",
       "3     [101, 5712, 9190, 2015, 3514, 14338, 2013, 236...   \n",
       "4     [101, 3514, 7597, 2061, 2906, 2000, 2035, 1011...   \n",
       "...                                                 ...   \n",
       "1195  [101, 8582, 3504, 2000, 8536, 2049, 11550, 105...   \n",
       "1196  [101, 9383, 1005, 1055, 3632, 2005, 2751, 2005...   \n",
       "1197  [101, 10470, 2005, 6661, 1999, 8224, 1005, 105...   \n",
       "1198  [101, 2176, 9786, 4965, 13420, 16846, 13420, 1...   \n",
       "1199  [101, 10470, 2005, 6661, 1999, 8224, 1005, 105...   \n",
       "\n",
       "                                         token_type_ids  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "1195  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1196  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1197  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1198  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1199  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         attention_mask  \n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                 ...  \n",
       "1195  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1196  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1197  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1198  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1199  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[1200 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss() # Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/150 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: batch[key]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2865\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[1;32m   2866\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py:2846\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2848\u001b[0m )\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py:641\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m     pa_table_to_format \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mdrop(col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m format_columns)\n\u001b[0;32m--> 641\u001b[0m     formatted_output \u001b[38;5;241m=\u001b[39m formatter(pa_table_to_format, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/torch_formatter.py:110\u001b[0m, in \u001b[0;36mTorchFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping:\n\u001b[0;32m--> 110\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    111\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    112\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(batch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py:165\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py:165\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py:197\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    192\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x))\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 종료 이후 numpy 복귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda uninstall numpy\n",
    "%conda install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
