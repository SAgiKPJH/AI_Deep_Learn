{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI/ML 서비스 개발 과정\n",
    "\n",
    "- 데이터 수집 및 전처리\n",
    "  - 예) pandas\n",
    "- 모델 학습 및 평가\n",
    "  - 예) pytorch\n",
    "- [본과정] 모델 패키징 및 서빙\n",
    "  - 예) pytorch serving, BentoML, Triton Server, Kubeflow KServe\n",
    "- 모니터링 및 학습 파이프라인, 피쳐스토어\n",
    "  - 예) k8s\n",
    "\n",
    "\n",
    "### 모델 배포 과정\n",
    "- 데이터 -> 모델 아키텍처 -> 모델 (checkpoint) -> 모델 레지스트리 -> 모델 서빙 (Docker)\n",
    "\n",
    "\n",
    "### 모델 배포 유형\n",
    "- On-premise\n",
    "  - Local\n",
    "  - CUDA, CUDNN 설정 필요\n",
    "  - k8s 구축\n",
    "- Cloud 서빙\n",
    "  - 클라우드 환경\n",
    "  - 구글 클라우드: Vertex AI / SageMarker등을 통한 서빙\n",
    "- Edge-device 서빙\n",
    "  - 모바일 단말기, Jetson, Coral등 모델 서빙\n",
    "  - Onxx, TensorLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchServ를 통한 모델 서빙\n",
    "\n",
    "- TorchServe와 Django, FastAPI 차이점\n",
    "  - 딥러닝 서버에 특화됨\n",
    "  - Multi Process, Deep Speed MII등으로 거대 모델 인퍼런스 성능 향상\n",
    "\n",
    "\n",
    "### 배포 과정\n",
    "- 모델 학습 및 평가: HuggingFace -(데이터셋/모델 다운로드)-> Pytorch Training -(모델 패키징 *.mar)->\n",
    "- 모델 패키징: model-archiver -(TrochServe 모델 서빙)->\n",
    "- 모델 서빙: torchserve -(Colab-Local 연결)-> ngrok tunneling\n",
    "\n",
    "\n",
    "### Colab\n",
    "- https://colab.research.google.com/drive/1fEOF1RRX5CPeFmgfNYq1ihQQsa71LMNq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets torchserve torch-model-archiver torch-workflow-archiver nvgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 모델 학습 및 평가 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BatchEncoding, BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class DatasetItem(TypedDict):\n",
    "    text: str\n",
    "    label: str\n",
    "\n",
    "\n",
    "def preprocess_data(dataset_item: DatasetItem) -> dict[str, torch.Tensor]:\n",
    "    return tokenizer(dataset_item[\"text\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(1200)).map(preprocess_data, batched=True)\n",
    "test_dataset = dataset[\"test\"].select(range(800)).map(preprocess_data, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "losses: list[float] = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses, color=\"#fc1c49\", linewidth=2)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss per Step Across Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_predictions: list[int] = []\n",
    "all_labels: list[int] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        inputs = {key: batch[key].to(device) for key in batch}\n",
    "        labels = inputs.pop(\"label\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"g\", cmap=sns.light_palette(\"#fc1c49\", as_cmap=True))\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 서빙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 체크포인트 저장.\n",
    "model_save_path = \"bert_news_classification_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model_handler.py\n",
    "\n",
    "%%shell\n",
    "cat > model_handler.py <<EOF\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from ts.context import Context\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "from transformers import BatchEncoding, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class ModelHandler(BaseHandler):\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def initialize(self, context: Context):\n",
    "        self.initialized = True\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "        self.model.load_state_dict(torch.load(\"bert_news_classification_model.pth\"))\n",
    "        self.model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess(self, data: list[dict[str, bytearray]]) -> BatchEncoding:\n",
    "        model_input_texts: list[str] = sum([json.loads(item.get(\"body\").decode(\"utf-8\"))[\"data\"] for item in data], [])\n",
    "        inputs = self.tokenizer(model_input_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return inputs.to(device)\n",
    "\n",
    "    def inference(self, input_batch: BatchEncoding) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**input_batch)\n",
    "            return outputs.logits\n",
    "\n",
    "    def postprocess(self, inference_output: torch.Tensor) -> list[dict[str, float]]:\n",
    "        probabilities = torch.nn.functional.softmax(inference_output, dim=1)\n",
    "        return [{\"label\": int(torch.argmax(prob)), \"probability\": float(prob.max())} for prob in probabilities]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# torch serve 설정 파일.\n",
    "\n",
    "%%shell\n",
    "cat > config.properties <<EOF\n",
    "inference_address=http://0.0.0.0:5000\n",
    "management_address=http://0.0.0.0:5001\n",
    "metrics_address=http://0.0.0.0:5002\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# bert vocab 파일 (아티팩트)\n",
    "\n",
    "!wget https://raw.githubusercontent.com/microsoft/SDNet/master/bert_vocab_files/bert-base-uncased-vocab.txt \\\n",
    "    -O bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Torch serve archiver의 최종 파일 저장 위치.\n",
    "# .mar 확장자 파일을 생성하여 패키징 하게 된다.\n",
    "\n",
    "!mkdir -p model-store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "    --model-name bert_news_classification \\\n",
    "    --version 1.0 \\\n",
    "    --serialized-file bert_news_classification_model.pth \\\n",
    "    --handler ./model_handler.py \\\n",
    "    --extra-files \"bert-base-uncased-vocab.txt\" \\\n",
    "    --export-path model-store \\\n",
    "    -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서빙 시작\n",
    "\n",
    "- 중지는 다음과 같이 진행\n",
    "```python\n",
    "# 서버 중지\n",
    "torchserve --stop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%script bash --bg\n",
    "# 서버를 백그라운드에서 실행\n",
    "\n",
    "PYTHONPATH=/usr/lib/python3.10 torchserve \\\n",
    "    --start \\\n",
    "    --ncs \\\n",
    "    --ts-config config.properties \\\n",
    "    --model-store model-store \\\n",
    "    --models bert_news_classification=bert_news_classification.mar # Local은 n% 플래그 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 아래와 같이 뜨면 정상 실행.\n",
    "# {\n",
    "#   \"status\": \"Healthy\"\n",
    "# }\n",
    "\n",
    "!curl -X GET localhost:5000/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서빙 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# 모델 실제 평가를 위해 외부 뉴스 기사 데이터 셋 파일 생성.\n",
    "\n",
    "cat > request_sports.json <<EOF\n",
    "{\n",
    "  \"data\": [\n",
    "    \"Bleary-eyed from 16 hours on a Greyhound bus, he strolled into the stadium running on fumes. He’d barely slept in two days. The ride he was supposed to hitch from Charlotte to Indianapolis canceled at the last minute, and for a few nervy hours, Antonio Barnes started to have his doubts. The trip he’d waited 40 years for looked like it wasn’t going to happen.ADVERTISEMENTBut as he moved through the concourse at Lucas Oil Stadium an hour before the Colts faced the Raiders, it started to sink in. His pace quickened. His eyes widened. His voice picked up.“I got chills right now,” he said. “Chills.”Barnes, 57, is a lifer, a Colts fan since the Baltimore days. He wore No. 25 on his pee wee football team because that’s the number Nesby Glasgow wore on Sundays. He was a talent in his own right, too: one of his old coaches nicknamed him “Bird” because of his speed with the ball.Back then, he’d catch the city bus to Memorial Stadium, buy a bleacher ticket for $5 and watch Glasgow and Bert Jones, Curtis Dickey and Glenn Doughty. When he didn’t have any money, he’d find a hole in the fence and sneak in. After the game was over, he’d weasel his way onto the field and try to meet the players. “They were tall as trees,” he remembers.He remembers the last game he went to: Sept. 25, 1983, an overtime win over the Bears. Six months later the Colts would ditch Baltimore in the middle of the night, a sucker-punch some in the city never got over. But Barnes couldn’t quit them. When his entire family became Ravens fans, he refused. “The Colts are all I know,” he says.For years, when he couldn’t watch the games, he’d try the radio. And when that didn’t work, he’d follow the scroll at the bottom of a screen.“There were so many nights I’d just sit there in my cell, picturing what it’d be like to go to another game,” he says. “But you’re left with that thought that keeps running through your mind: I’m never getting out.”It’s hard to dream when you’re serving a life sentence for conspiracy to commit murder.It started with a handoff, a low-level dealer named Mickey Poole telling him to tuck a Ziploc full of heroin into his pocket and hide behind the Murphy towers. This was how young drug runners were groomed in Baltimore in the late 1970s. This was Barnes’ way in.ADVERTISEMENTHe was 12.Back then he idolized the Mickey Pooles of the world, the older kids who drove the shiny cars, wore the flashy jewelry, had the girls on their arms and made any working stiff punching a clock from 9 to 5 look like a fool. They owned the streets. Barnes wanted to own them, too.“In our world,” says his nephew Demon Brown, “the only successful people we saw were selling drugs and carrying guns.”So whenever Mickey would signal for a vial or two, Barnes would hurry over from his hiding spot with that Ziploc bag, out of breath because he’d been running so hard.\"\n",
    "  ]\n",
    "}\n",
    "EOF\n",
    "\n",
    "cat > request_business.json <<EOF\n",
    "{\n",
    "  \"data\": [\n",
    "    \"DETROIT – America maintained its love affair with pickup trucks in 2023 — but a top-selling vehicle from Toyota Motor nearly ruined their tailgate party.Sales of the Toyota RAV4 compact crossover came within 10,000 units of Stellantis’ Ram pickup truck last year, a near-No. 3 ranking that would have marked the first time since 2014 that a non-pickup claimed one of the top three U.S. sales podium positions.The RAV4 has rapidly closed the gap: In 2020, the vehicle undersold the Ram truck by more than 133,000 units. Last year, it lagged by just 9,983. Stellantis sold 444,926 Ram pickups last year, a 5% decline from 2022.“Trucks are always at the top because they’re bought by not only individuals, but also fleet buyers and we saw heavy fleet buying last year,” said Michelle Krebs, an executive analyst at Cox Automotive. “The RAV4 shows that people want affordable, smaller SUVs, and the fact that there’s also a hybrid version of that makes it popular with people.”\"\n",
    "  ]\n",
    "}\n",
    "EOF\n",
    "\n",
    "cat > request_sci_tech.json <<EOF\n",
    "{\n",
    "  \"data\": [\n",
    "    \"OpenVoice comprises two AI models working together for text-to-speech conversion and voice tone cloning.The first model handles language style, accents, emotion, and other speech patterns. It was trained on 30,000 audio samples with varying emotions from English, Chinese, and Japanese speakers. The second “tone converter” model learned from over 300,000 samples encompassing 20,000 voices.By combining the universal speech model with a user-provided voice sample, OpenVoice can clone voices with very little data. This helps it generate cloned speech significantly faster than alternatives like Meta’s Voicebox.Californian startup OpenVoice comes from California-based startup MyShell, founded in 2023. With $5.6 million in early funding and over 400,000 users already, MyShell bills itself as a decentralised platform for creating and discovering AI apps.  In addition to pioneering instant voice cloning, MyShell offers original text-based chatbot personalities, meme generators, user-created text RPGs, and more. Some content is locked behind a subscription fee. The company also charges bot creators to promote their bots on its platform.By open-sourcing its voice cloning capabilities through HuggingFace while monetising its broader app ecosystem, MyShell stands to increase users across both while advancing an open model of AI development.\"\n",
    "  ]\n",
    "}\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 스포츠 기사 평가 (레이블: 1)\n",
    "!curl -X POST \\\n",
    "    -H \"Accept: application/json\" \\\n",
    "    -T \"request_sports.json\" \\\n",
    "    http://localhost:5000/predictions/bert_news_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 비즈니스 기사 평가 (레이블: 2)\n",
    "!curl -X POST \\\n",
    "    -H \"Accept: application/json\" \\\n",
    "    -T \"request_business.json\" \\\n",
    "    http://localhost:5000/predictions/bert_news_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 테크 기사 평가 (레이블: 3)\n",
    "!curl -X POST \\\n",
    "    -H \"Accept: application/json\" \\\n",
    "    -T \"request_sci_tech.json\" \\\n",
    "    http://localhost:5000/predictions/bert_news_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrok을 통한 외부 연결\n",
    "\n",
    "- https://ngrok.com/\n",
    "- 가입 후 `귀하의 인증 토큰` 접속"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok.set_auth_token(\"2aT0pKF7B5b3u8cQbsDoj12qdZs_52zPrJtqRW9dAGC3w6VFW\") # ngrok 가입 후 토큰 기입\n",
    "inference_tunnel = ngrok.connect(\"5000\")\n",
    "inference_tunnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local 실행\n",
    "```bash\n",
    "curl -X POST \\\n",
    "    -H \"Accept: application/json\" \\\n",
    "    -T \"request_sci_tech.json\" \\\n",
    "    https://1c1d-34-29-238-199.ngrok-free.app/predictions/bert_news_classification\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
