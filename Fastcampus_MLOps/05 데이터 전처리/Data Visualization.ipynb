{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 및 시각화\n",
    "- Pandas, Matplotlib 활용\n",
    "- Pandas\n",
    "  - 2008년 출시, Wes McKineey 개발\n",
    "  - 목적 : 데이터 분석 및 조작을 위한 고성능 처리(Pandas Core가 c언어로 처리되어 있다), 데이터 구조를 지원하는 라이브러리\n",
    "  - 특징 : 데이터 프레임(DataFrame) 및 시리즈(Series)를 바탕으로 데이터 처리\n",
    "    1. 데이터 처리 빠르고 효율적\n",
    "    2. 지원하는 생태계가 넓어 다른 라이브러리와 연결/통합 가능\n",
    "    3. 인덱싱 지원, 효율적인 처리 가능\n",
    "    4. 벡터화된 연산 지원\n",
    "    5. Numpy를 기반, Numpy 연산 대부분 호환\n",
    "  - Series는 한 Column과 데이터를 함께 묶은 개념\n",
    "\n",
    "### EDA\n",
    "- EDA?\n",
    "  - EDA(Exploratory Data Analysis), 탐색적 데이터 분석\n",
    "- Colab에서는 Pandas 관련 간단한 EDA가 존재\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2025.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age      City\n",
       "0    Alice   25  New York\n",
       "1      Bob   30     Paris\n",
       "2  Charlie   35    London"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 35],\n",
    "    \"City\": [\"New York\", \"Paris\", \"London\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"sample_data.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   70000\n",
      "1      Bob   30     Paris   80000\n",
      "2  Charlie   35    London   90000\n",
      "\n",
      "Age Column: \n",
      "0    25\n",
      "1    30\n",
      "2    35\n",
      "Name: Age, dtype: int64\n",
      "\n",
      "Filtered DataFrame (Age > 28):\n",
      "      Name  Age    City  Salary\n",
      "1      Bob   30   Paris   80000\n",
      "2  Charlie   35  London   90000\n",
      "\n",
      "DataFrame with Salary column:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   70000\n",
      "1      Bob   30     Paris   80000\n",
      "2  Charlie   35    London   90000\n"
     ]
    }
   ],
   "source": [
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nAge Column: \")\n",
    "print(df[\"Age\"])\n",
    "\n",
    "\n",
    "filtered_df = df[df[\"Age\"] > 28]\n",
    "print(\"\\nFiltered DataFrame (Age > 28):\")\n",
    "print(filtered_df)\n",
    "\n",
    "df[\"Salary\"] = [70000, 80000, 90000]\n",
    "print(\"\\nDataFrame with Salary column:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"Age\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 변환 및 집계\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "  - Pandas 관련 내용 간단 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "data = {\n",
    "    \"Name\": np.random.choice([\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", None], size=1000),\n",
    "    \"Age\": np.random.randint(20, 60, size=1000),\n",
    "    \"City\":np.random.choice([\"Seoul\", \"New York\", \"Paris\", \"London\", \"Berlin\", \"Tokyo\"], size=1000),\n",
    "    \"Salary\": np.random.randint(50000, 120000, size=1000),\n",
    "}\n",
    "\n",
    "departments = [\"HR\", \"Marketing\", \"Sales\", \"IT\", \"Finance\"]\n",
    "np.random.seed(0)\n",
    "df2 = pd.DataFrame({\n",
    "    \"Name\": df[\"Name\"].dropna().unique(), # unique: 중복 제거\n",
    "    \"Department\": np.random.choice(departments, size=len(df[\"Name\"].dropna().unique()))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age      City  Salary\n",
      "0      Eve   35     Seoul   50889\n",
      "1      NaN   40    Berlin  110224\n",
      "2    Alice   29     Tokyo   80141\n",
      "3    David   24     Paris   59740\n",
      "4    David   44     Paris   62998\n",
      "..     ...  ...       ...     ...\n",
      "995    Eve   24  New York   77062\n",
      "996    NaN   30     Tokyo   76061\n",
      "997    NaN   26     Tokyo   55645\n",
      "998  David   28     Seoul  110482\n",
      "999  Alice   28    Berlin   61768\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "      Name  Age      City  Salary\n",
      "0      Eve   35     Seoul   50889\n",
      "1      NaN   40    Berlin  110224\n",
      "2    Alice   29     Tokyo   80141\n",
      "3    David   24     Paris   59740\n",
      "4    David   44     Paris   62998\n",
      "..     ...  ...       ...     ...\n",
      "995    Eve   24  New York   77062\n",
      "996    NaN   30     Tokyo   76061\n",
      "997    NaN   26     Tokyo   55645\n",
      "998  David   28     Seoul  110482\n",
      "999  Alice   28    Berlin   61768\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "      Name  Age      City  Salary\n",
      "0      Eve   35     Seoul   50889\n",
      "1     None   40    Berlin  110224\n",
      "2    Alice   29     Tokyo   80141\n",
      "3    David   24     Paris   59740\n",
      "4    David   44     Paris   62998\n",
      "..     ...  ...       ...     ...\n",
      "995    Eve   24  New York   77062\n",
      "996   None   30     Tokyo   76061\n",
      "997   None   26     Tokyo   55645\n",
      "998  David   28     Seoul  110482\n",
      "999  Alice   28    Berlin   61768\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 저장/불러오기\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"sample_data.csv\", index=False)\n",
    "df2.to_csv(\"sample_data2.csv\", index=False)\n",
    "df.to_excel(\"sample_data.xlsx\", index=False) # 3만개 정도 넘어가면 저장 못할 수 있다 (Microsoft excel의 한계)\n",
    "df.to_json(\"sample_data.json\", index=False) # 정보 처리 중간에 오류 발생하면 처리 불가\n",
    "df.to_json(\"sample_data_multilines.json\", orient=\"records\", index=False, lines=True) # 오류 발생 개선\n",
    "\n",
    "df = pd.read_excel(\"sample_data.xlsx\")\n",
    "print(df)\n",
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "print(df)\n",
    "df = pd.read_json(\"sample_data.json\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name  Age      City  Salary\n",
      "0        Eve   35     Seoul   50889\n",
      "1        NaN   40    Berlin  110224\n",
      "4      David   44     Paris   62998\n",
      "5      David   50     Tokyo  104176\n",
      "7      David   39  New York  117249\n",
      "..       ...  ...       ...     ...\n",
      "989  Charlie   37  New York  116816\n",
      "990      NaN   42    London   81501\n",
      "991      Eve   33  New York   75625\n",
      "993      NaN   31     Tokyo   60565\n",
      "996      NaN   30     Tokyo   76061\n",
      "\n",
      "[769 rows x 4 columns]\n",
      "   Name  Age   City  Salary\n",
      "0   Eve   35  Seoul   50889\n",
      "17  Bob   41  Seoul  119063\n",
      "18  NaN   40  Seoul   67344\n",
      "20  NaN   57  Seoul   85779\n",
      "32  Bob   45  Seoul  108539\n",
      "      Name  Age      City  Salary\n",
      "3    David   24     Paris   59740\n",
      "4    David   44     Paris   62998\n",
      "5    David   50     Tokyo  104176\n",
      "7    David   39  New York  117249\n",
      "9  Charlie   33    Berlin   50075\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정제\n",
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "\n",
    "older_than30 = df[df[\"Age\"] >= 30]\n",
    "print(older_than30)\n",
    "\n",
    "older_in_seoul = df[(df[\"Age\"] >= 30) & (df[\"City\"] == \"Seoul\")]\n",
    "print(older_in_seoul.head()) # 5개만 출력\n",
    "\n",
    "df_not_null = df[df[\"Name\"].notna()]\n",
    "df_is_null = df[df[\"Name\"].isna()]\n",
    "\n",
    "# 이름에 'a' 포함\n",
    "df_with_a = df[df[\"Name\"].str.contains(\"a\", na=False)]\n",
    "print(df_with_a.head()) # 5개만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2      False\n",
       "3      False\n",
       "4       True\n",
       "       ...  \n",
       "995    False\n",
       "996     True\n",
       "997    False\n",
       "998    False\n",
       "999    False\n",
       "Name: Age, Length: 1000, dtype: bool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Age\"] >= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age      City  Salary\n",
      "0      EVE   35     Seoul   50889\n",
      "1      NaN   40    Berlin  110224\n",
      "2    ALICE   29     Tokyo   80141\n",
      "3    DAVID   24     Paris   59740\n",
      "4    DAVID   44     Paris   62998\n",
      "..     ...  ...       ...     ...\n",
      "995    EVE   24  New York   77062\n",
      "996    NaN   30     Tokyo   76061\n",
      "997    NaN   26     Tokyo   55645\n",
      "998  DAVID   28     Seoul  110482\n",
      "999  ALICE   28    Berlin   61768\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "      Name  Age      City  Salary            Name_City\n",
      "0      EVE   35     Seoul   50889         EVE from EVE\n",
      "1      NaN   40    Berlin  110224  Unknown from Berlin\n",
      "2    ALICE   29     Tokyo   80141     ALICE from ALICE\n",
      "3    DAVID   24     Paris   59740     DAVID from DAVID\n",
      "4    DAVID   44     Paris   62998     DAVID from DAVID\n",
      "..     ...  ...       ...     ...                  ...\n",
      "995    EVE   24  New York   77062         EVE from EVE\n",
      "996    NaN   30     Tokyo   76061   Unknown from Tokyo\n",
      "997    NaN   26     Tokyo   55645   Unknown from Tokyo\n",
      "998  DAVID   28     Seoul  110482     DAVID from DAVID\n",
      "999  ALICE   28    Berlin   61768     ALICE from ALICE\n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 변환\n",
    "\n",
    "# 예) 대분자 변환\n",
    "df[\"Name\"] = df[\"Name\"].apply(lambda x: x.upper() if pd.notna(x) else x)\n",
    "print(df)\n",
    "\n",
    "# 예) 이름과 City를 결합한 새로운 컬럼 생성\n",
    "def process(row: pd.Series) -> str:\n",
    "    if pd.notna(row['Name']):\n",
    "        return f\"{row['Name']} from {row['Name']}\"\n",
    "    return f\"Unknown from {row['City']}\"\n",
    "df[\"Name_City\"] = df.apply(process, axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84173.79\n",
      "City\n",
      "Berlin      40.493976\n",
      "London      39.914439\n",
      "New York    39.987261\n",
      "Paris       38.596491\n",
      "Seoul       40.320261\n",
      "Tokyo       38.686747\n",
      "Name: Age, dtype: float64\n",
      "         Age     Salary\n",
      "mean  39.655   84173.79\n",
      "min   20.000   50010.00\n",
      "max   59.000  119939.00\n",
      "      Name  Age      City  Salary Department\n",
      "0      Eve   35     Seoul   50889    Finance\n",
      "1    Alice   29     Tokyo   80141         HR\n",
      "2    David   24     Paris   59740         IT\n",
      "3    David   44     Paris   62998         IT\n",
      "4    David   50     Tokyo  104176         IT\n",
      "..     ...  ...       ...     ...        ...\n",
      "824    Bob   22     Paris   59307         IT\n",
      "825  David   29     Tokyo   82691         IT\n",
      "826    Eve   24  New York   77062    Finance\n",
      "827  David   28     Seoul  110482         IT\n",
      "828  Alice   28    Berlin   61768         HR\n",
      "\n",
      "[829 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 집계/조인\n",
    "average_salary = df[\"Salary\"].mean()\n",
    "print(average_salary)\n",
    "\n",
    "average_age_by_city = df.groupby(\"City\")[\"Age\"].mean()\n",
    "print(average_age_by_city)\n",
    "\n",
    "aggregated_data = df.agg({\"Age\": [\"mean\", \"min\", \"max\"], \"Salary\": [\"mean\", \"min\", \"max\"]})\n",
    "print(aggregated_data)\n",
    "\n",
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "df2 = pd.read_csv(\"sample_data2.csv\")\n",
    "# Inner Join (일치된 것들만 병합)\n",
    "inner_joined = pd.merge(df, df2, on=\"Name\", how=\"inner\")\n",
    "print(inner_joined)\n",
    "\n",
    "# Outer Join\n",
    "outer_joined = pd.merge(df, df2, on=\"Name\", how=\"outer\") # 예시에서는 Name=NaN인 경우에 Department = NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask, 대용량 데이터 처리\n",
    "- Dask\n",
    "  - 2014년 출시, Matthew Rocklin 개발\n",
    "  - 목적: 대규모 계산을 위한 병렬 처리, 지연 실행 및 대규모 데이터셋 처리\n",
    "  - 특징\n",
    "    - 동적 작업 스케쥴링\n",
    "    - 대규모 배열\n",
    "    - 데이터 프레임\n",
    "    - 리스트 처리\n",
    "    - 머신 러닝\n",
    "    - 병렬 처리 지원\n",
    "    - Pandas 호환 API\n",
    "  - 구조\n",
    "    - 자료구조\n",
    "      - Dask Array\n",
    "      - Dask DataFrame\n",
    "      - Dask Bag\n",
    "      - Dask Delayed\n",
    "      - Futures\n",
    "    - 작업 그래프\n",
    "    - 스케줄러\n",
    "      - 단일 머신 (thread, process, 동기화)\n",
    "      - 분산 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /usr/local/lib/python3.11/site-packages (2025.1.0)\n",
      "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/site-packages (from dask) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/site-packages (from dask) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from dask) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/site-packages (from dask) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/site-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.11/site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/site-packages (from pyarrow) (2.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dask\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m main_df, ref_df \u001b[38;5;241m=\u001b[39m create_dataset(\u001b[38;5;241m10_000_000\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     43\u001b[0m pandas_agg_time, pandas_join_time \u001b[38;5;241m=\u001b[39m pandas_operations(main_df, ref_df)\n\u001b[0;32m---> 44\u001b[0m dask_agg_time, dask_join_time \u001b[38;5;241m=\u001b[39m dask_operations(main_df, ref_df, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas 집계 시간: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pandas_agg_time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m초\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas 조인 시간: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pandas_join_time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m초\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[63], line 25\u001b[0m, in \u001b[0;36mdask_operations\u001b[0;34m(main_df, ref_df, npartitions)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdask_operations\u001b[39m(main_df: pd\u001b[38;5;241m.\u001b[39mDataFrame, ref_df: pd\u001b[38;5;241m.\u001b[39mDataFrame, npartitions: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 25\u001b[0m     dmain_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(main_df, npartitions\u001b[38;5;241m=\u001b[39mnpartitions)\n\u001b[1;32m     26\u001b[0m     dref_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(ref_df, npartitions\u001b[38;5;241m=\u001b[39mnpartitions)\n\u001b[1;32m     28\u001b[0m     start_time_agg \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:4913\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[0;34m(data, npartitions, sort, chunksize)\u001b[0m\n\u001b[1;32m   4907\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide chunksize as an int, or possibly as None if you specify npartitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4909\u001b[0m     )\n\u001b[1;32m   4911\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FromPandas\n\u001b[0;32m-> 4913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m   4914\u001b[0m     FromPandas(\n\u001b[1;32m   4915\u001b[0m         _BackendData(data\u001b[38;5;241m.\u001b[39mcopy()),\n\u001b[1;32m   4916\u001b[0m         npartitions\u001b[38;5;241m=\u001b[39mnpartitions,\n\u001b[1;32m   4917\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   4918\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m   4919\u001b[0m         pyarrow_strings_enabled\u001b[38;5;241m=\u001b[39mpyarrow_strings_enabled(),\n\u001b[1;32m   4920\u001b[0m     )\n\u001b[1;32m   4921\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:4814\u001b[0m, in \u001b[0;36mnew_collection\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m   4812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[1;32m   4813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4814\u001b[0m     meta \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   4815\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[1;32m   4816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dask/dataframe/dask_expr/io/io.py:434\u001b[0m, in \u001b[0;36mFromPandas._meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyarrow_strings_enabled:\n\u001b[0;32m--> 434\u001b[0m         meta \u001b[38;5;241m=\u001b[39m make_meta(to_pyarrow_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dask/dataframe/_pyarrow.py:61\u001b[0m, in \u001b[0;36m_to_string_dtype\u001b[0;34m(df, dtype_check, index_check, string_dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Guards against importing `pyarrow` at the module level (where it may not be installed)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m string_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     string_dtype \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mStringDtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Possibly convert DataFrame/Series/Index to `string[pyarrow]`\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(df):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/arrays/string_.py:131\u001b[0m, in \u001b[0;36mStringDtype.__init__\u001b[0;34m(self, storage)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow_numpy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow_numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pa_version_under10p1:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow>=10.0.1 is required for PyArrow backed StringArray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage \u001b[38;5;241m=\u001b[39m storage\n",
      "\u001b[0;31mImportError\u001b[0m: pyarrow>=10.0.1 is required for PyArrow backed StringArray."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def create_dataset(nrows: int, ncols: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    main_data = {f\"col_{i}\": np.random.rand(nrows) for i in range(ncols)}\n",
    "    ref_data = {f\"col_{i}\": np.random.rand(nrows // 10) for i in range(ncols)}\n",
    "    main_df = pd.DataFrame(main_data)\n",
    "    ref_df = pd.DataFrame(ref_data)\n",
    "    return main_df, ref_df\n",
    "\n",
    "def pandas_operations(main_df: pd.DataFrame, ref_df: pd.DataFrame) -> tuple[float, float]:\n",
    "    start_time_agg = time.time()\n",
    "    grouped = main_df.groupby(\"col_0\").mean()\n",
    "    end_time_agg = time.time()\n",
    "\n",
    "    start_time_join = time.time()\n",
    "    joined = main_df.merge(ref_df, on=\"col_0\", how=\"left\")\n",
    "    end_time_join = time.time()\n",
    "\n",
    "    return end_time_agg - start_time_agg, end_time_join - start_time_join\n",
    "\n",
    "def dask_operations(main_df: pd.DataFrame, ref_df: pd.DataFrame, npartitions: int) -> tuple[float, float]:\n",
    "    dmain_df = dd.from_pandas(main_df, npartitions=npartitions)\n",
    "    dref_df = dd.from_pandas(ref_df, npartitions=npartitions)\n",
    "\n",
    "    start_time_agg = time.time()\n",
    "    grouped_task = dmain_df.groupby(\"col_0\").mean()\n",
    "    grouped = grouped_task.compute()\n",
    "    end_time_agg = time.time()\n",
    "    grouped_task.visualize(\"grouped.svg\")\n",
    "\n",
    "    start_time_join = time.time()\n",
    "    joined_task = dmain_df.merge(dref_df, on=\"col_0\", how=\"left\")\n",
    "    joined = joined_task.compute()\n",
    "    end_time_join = time.time()\n",
    "    joined_task.visualize(\"joined.svg\")\n",
    "\n",
    "    return end_time_agg - start_time_agg, end_time_join - start_time_join\n",
    "\n",
    "main_df, ref_df = create_dataset(10_000_000, 5)\n",
    "pandas_agg_time, pandas_join_time = pandas_operations(main_df, ref_df)\n",
    "dask_agg_time, dask_join_time = dask_operations(main_df, ref_df, npartitions=10)\n",
    "\n",
    "print(\"Pandas 집계 시간: \", pandas_agg_time, \"초\")\n",
    "print(\"Pandas 조인 시간: \", pandas_join_time, \"초\")\n",
    "print(\"Dask 집계 시간: \", dask_agg_time, \"초\")\n",
    "print(\"Dask 조인 시간: \", dask_join_time, \"초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
